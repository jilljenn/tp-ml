{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%mkdir logs\n",
        "!wget http://files.fast.ai/data/examples/dogscats.tgz\n",
        "!tar xzf dogscats.tgz"
      ],
      "metadata": {
        "id": "U5VzgEXa1G4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQjVO7Xm0Qet"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "{CNN or perceptron} on {faces or dogs/cats} datasets\n",
        "JJV for Deep Learning course, 2022\n",
        "\"\"\"\n",
        "from datetime import datetime\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torchvision import transforms, datasets\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm  # Nice progress bars\n",
        "\n",
        "\n",
        "# DATA = 'catsdogs'\n",
        "DATA = 'faces'\n",
        "N_EPOCHS = 30 if DATA == 'faces' else 2\n",
        "LEARNING_RATE = 0.01\n",
        "BATCH_SIZE = 100\n",
        "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "TIME = datetime.now().strftime(\"%Hh%Mm%Ss\")\n",
        "\n",
        "\n",
        "def preprocess_data():\n",
        "    '''\n",
        "    Prepare datasets.\n",
        "    Perform various operations (matrix rotation, normalization),\n",
        "    then split into train and test datasets.\n",
        "    Returns iterators over train and test.\n",
        "    '''\n",
        "    if DATA == 'catsdogs':\n",
        "        data_transform = transforms.Compose([\n",
        "            transforms.CenterCrop(224),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        train_dataset = datasets.ImageFolder(\n",
        "            root='dogscats/train', transform=data_transform)\n",
        "        test_dataset = datasets.ImageFolder(\n",
        "            root='dogscats/valid', transform=data_transform)\n",
        "        # plt.imshow(train_dataset[0][0].permute(1, 2, 0).numpy())  # Display\n",
        "        # plt.show()\n",
        "        input_shape = (3, 224, 224)\n",
        "        reduced_shape = (32, 27, 27)\n",
        "    else:\n",
        "        faces = fetch_lfw_people(min_faces_per_person=70, color=True)\n",
        "        # plt.imshow(faces.images[0])  # Display one image\n",
        "        # plt.show()\n",
        "        X = torch.Tensor(faces.images).permute(0, 3, 1, 2)\n",
        "        y = torch.LongTensor(faces.target)\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y, test_size=0.2, shuffle=True)\n",
        "        train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "        test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "        input_shape = (3, 62, 47)\n",
        "        reduced_shape = (32, 7, 5)\n",
        "\n",
        "    train_iter = torch.utils.data.DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    test_iter = torch.utils.data.DataLoader(test_dataset, batch_size=100)\n",
        "    return train_iter, test_iter, input_shape, reduced_shape\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN or perceptron (one fully connected layer).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape, reduced_shape, with_cnn=False):\n",
        "        super().__init__()\n",
        "        self.with_cnn = with_cnn\n",
        "        self.input_shape = input_shape\n",
        "        self.reduced_shape = reduced_shape\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=(3, 3)),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=(8, 8)),\n",
        "            # nn.Conv2d(32, 64, (3, 3)),\n",
        "            # nn.ReLU(),\n",
        "            # nn.MaxPool2d((4, 4)),\n",
        "        )\n",
        "        self.fully_connected_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(in_features=np.prod(self.reduced_shape) if with_cnn\n",
        "                      else np.prod(self.input_shape), out_features=7),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.with_cnn:\n",
        "            x = self.conv_layers(x)\n",
        "        logits = self.fully_connected_layers(x)\n",
        "        return logits\n",
        "\n",
        "    def __str__(self):\n",
        "        return 'cnn' if self.with_cnn else 'perceptron'\n",
        "\n",
        "\n",
        "def train(dataloader, model, loss_function, optimizer, writer):\n",
        "    model.train()  # Training mode\n",
        "    losses = []\n",
        "    accuracies = [0.]\n",
        "    for inputs, targets in tqdm(dataloader):\n",
        "        inputs = inputs.to(DEVICE)\n",
        "        targets = targets.to(DEVICE)\n",
        "        writer.add_graph(model, inputs)  # Display graph in TensorBoard\n",
        "        # writer.add_images('images', inputs)  # Display images in TensorBoard\n",
        "\n",
        "        # Compute prediction error\n",
        "        logits = model(inputs)\n",
        "        loss = loss_function(logits, targets)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        predictions = logits.argmax(axis=1)\n",
        "        accuracies.append(torch.sum(predictions == targets).item())\n",
        "\n",
        "        losses.append(loss.item())\n",
        "    return np.mean(losses), np.sum(accuracies) / len(dataloader.dataset)\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_function):\n",
        "    model.eval()  # Test mode\n",
        "    accuracies = []\n",
        "    with torch.no_grad():  # No training\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(DEVICE)\n",
        "            targets = targets.to(DEVICE)\n",
        "\n",
        "            logits = model(inputs)\n",
        "            predictions = logits.argmax(axis=1)\n",
        "            accuracies.append(torch.sum(predictions == targets).item())\n",
        "    return np.sum(accuracies) / len(dataloader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%rm -rf logs/fit"
      ],
      "metadata": {
        "id": "N7val-lDT9-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_iter, test_iter, input_shape, reduced_shape = preprocess_data()\n",
        "\n",
        "for with_cnn in (False, True):\n",
        "    model = NeuralNetwork(input_shape, reduced_shape, with_cnn).to(DEVICE)\n",
        "    writer = SummaryWriter(log_dir=f'logs/fit/{model}-{TIME}')  # TBoard\n",
        "\n",
        "    n_parameters = 0\n",
        "    for name, parameter in model.named_parameters():\n",
        "        print(name, parameter.numel())\n",
        "        n_parameters += parameter.numel()\n",
        "    print(f'Total number of parameters of {model}: {n_parameters}')\n",
        "\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        print(f'=== Epoch {epoch} ===')\n",
        "        train_loss, train_acc = train(train_iter, model, loss_function,\n",
        "                                      optimizer, writer)\n",
        "        print(f'Train loss: {train_loss:7f} / Train acc: {train_acc:.2f}')\n",
        "        writer.add_scalar('Loss/train', train_loss, epoch)\n",
        "        writer.add_scalar('Accuracy/train', train_acc, epoch)\n",
        "\n",
        "        test_acc = test(test_iter, model, loss_function)\n",
        "        print(f'Test accuracy: {test_acc:.2f}')\n",
        "        writer.add_scalar('Accuracy/test', test_acc, epoch)\n",
        "\n",
        "    writer.close()"
      ],
      "metadata": {
        "id": "fHcqkB1004A_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir logs/fit"
      ],
      "metadata": {
        "id": "LE3zOOBv0VBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9HJtug2I0pt-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}