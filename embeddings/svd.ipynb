{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6b86911",
   "metadata": {},
   "source": [
    "# Matrix completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb41d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Matrix completion on toy and Movielens datasets\n",
    "JJV for Deep Learning course, 2022\n",
    "\"\"\"\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "LEARNING_RATE = 0.1\n",
    "EMBEDDING_SIZE = 3\n",
    "\n",
    "\n",
    "DATA = 'toy'\n",
    "# DATA = 'movielens'\n",
    "if DATA == 'toy':\n",
    "    N_EPOCHS = 1000\n",
    "    DISPLAY_EPOCH_EVERY = 100\n",
    "    BATCH_SIZE = 50\n",
    "    N, K, M = 10, 3, 5\n",
    "    U = np.random.normal(size=(N, K))\n",
    "    V = np.random.normal(size=(M, K))\n",
    "    R = U @ V.T\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(N):  # Can be done using pd.unstack\n",
    "        for j in range(M):\n",
    "            X.append((i, N + j))\n",
    "            y.append(R[i, j])\n",
    "else:\n",
    "    N_EPOCHS = 50\n",
    "    DISPLAY_EPOCH_EVERY = 2\n",
    "    BATCH_SIZE = 1000\n",
    "    df = pd.read_csv('ml-latest-small/ratings.csv')\n",
    "    films = pd.read_csv('ml-latest-small/movies.csv')\n",
    "    df = df.merge(films, on='movieId')\n",
    "    df['user'] = np.unique(df['userId'], return_inverse=True)[1]\n",
    "    df['item'] = np.unique(df['movieId'], return_inverse=True)[1]\n",
    "    N = df['user'].nunique()\n",
    "    M = df['item'].nunique()\n",
    "    df['item'] += N\n",
    "    X = torch.LongTensor(df[['user', 'item']].to_numpy())\n",
    "    y = torch.Tensor(df['rating'])\n",
    "\n",
    "X = torch.LongTensor(X)\n",
    "y = torch.Tensor(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=True)\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "train_iter = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bc87be",
   "metadata": {},
   "source": [
    "- $X$ contains pairs of indices user and item. Please note that the data has been preprocessed so that user and item indices are disjoint: users are 0 to 610 and item IDs start at 611.\n",
    "- $y$ contains ratings between 1 and 5.\n",
    "\n",
    "## Toy dataset\n",
    "\n",
    "1. Write a first model using `nn.Sequential` that computes embeddings of size `EMBEDDING_SIZE` for both user and item considered; concatenates them, then feeds them to a linear layer for scalar prediction. Check `outputs.shape` to debug your code. Observe the train loss.\n",
    "\n",
    "2. Then modify the `CF` class to get a model that computes $u_i^T v_j$ for an $(i, j)$ user-item pair. What happens if `EMBEDDING_SIZE` is too high, say 20? When it is too small?\n",
    "\n",
    "## Movielens dataset\n",
    "\n",
    "1. Try the performance of both models on the Movielens dataset.\n",
    "\n",
    "2. We would like to combine both models: indeed, people don't have the same rating habits: some people rate between 0 and 2, some others between 4 and 5. Improve your model so that the predictions are:\n",
    "$$ f(i, j) = \\mu^U_{i} + \\mu^I_{j} + u_i^T v_j$$\n",
    "where $\\mu^U$ (resp. $\\mu^I$) is a vector of user (resp. item) biases, and u_i and v_j are embeddings of size `EMBEDDING_SIZE`.\n",
    "3. How would you implement L2 regularization? (Trick: use `weight_decay=1e-4` in your optimizer.) But without this trick, how would you do it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e415f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    ...  # Your code here\n",
    ")\n",
    "\n",
    "\n",
    "class CF(nn.Module):\n",
    "    \"\"\"\n",
    "    Recommender system\n",
    "    \"\"\"\n",
    "    def __init__(self, embedding_size):\n",
    "        super().__init__()\n",
    "        # Your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Your code here\n",
    "        pass\n",
    "\n",
    "\n",
    "# model = CF(EMBEDDING_SIZE)\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "losses = []\n",
    "for epoch in tqdm(range(N_EPOCHS)):\n",
    "    losses = []\n",
    "    for indices, target in train_iter:\n",
    "        outputs = model(indices).squeeze()\n",
    "        # print(outputs.shape)\n",
    "        loss = loss_function(outputs, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    if epoch % DISPLAY_EPOCH_EVERY == 0:\n",
    "        print(f\"Epoch {epoch}: Train MSE {np.mean(losses)}\")\n",
    "\n",
    "        y_pred = model(X_test).squeeze()\n",
    "        loss = loss_function(y_pred, y_test)\n",
    "        print('Test MSE', loss)\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir='logs/embeddings')  # TBoard\n",
    "item_embeddings = list(model.parameters())[1][N:]\n",
    "user_meta = pd.DataFrame(np.arange(N), columns=('item',))\n",
    "user_meta['title'] = ''\n",
    "item_meta = df.sort_values('item')[['item', 'title']].drop_duplicates()\n",
    "metadata = pd.concat((user_meta, item_meta), axis=0)\n",
    "writer.add_embedding(\n",
    "    item_embeddings, metadata=item_meta.values.tolist(),\n",
    "    metadata_header=item_meta.columns.tolist())\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
