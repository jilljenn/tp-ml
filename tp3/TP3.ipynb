{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP 3 - Dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Treasure maze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a maze.\n",
    "\n",
    "- Your robot can only go towards east and south (from top left).\n",
    "- On each case, it can collect some reward.\n",
    "- Find the path with maximum total reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 8  9 14  1 12 14]\n",
      " [ 6 16  5  4 10 18]\n",
      " [ 7  1  2  8 19 16]\n",
      " [ 5  4  0 14  7  3]\n",
      " [ 1  3  3 16  1  3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rewards = np.random.randint(20, size=(5, 6))\n",
    "N, M = rewards.shape\n",
    "print(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the greedy strategy optimal?\n",
    "\n",
    "After you've found the maximum total reward, please show the path your robot should take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = np.zeros((N, M))\n",
    "prec = [['.' for _ in range(M)] for _ in range(N)]\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Shortest paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[15  6  2  4  4]\n",
      " [18 19  8  2 14]\n",
      " [10 14 10 17 14]\n",
      " [ 2  8 18 13 13]\n",
      " [ 1 12 13  6 18]]\n"
     ]
    }
   ],
   "source": [
    "weights = np.random.randint(20, size=(5, 5))\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the shortest path between any pair of nodes. For that we will frame the **principle of optimality**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $s$ is a state, $a$ an action, $r$ a reward\n",
    "\n",
    "You cannot control the environment:\n",
    "\n",
    "- $p(s', r|s, a)$ is the probability to land in state $s'$ with reward $r$ when choosing action $a$ from state $s$.\n",
    "\n",
    "But you can control the agent:\n",
    "\n",
    "- $\\pi(a|s)$ is the probability that the agent chooses action $a$ from state $s$.\n",
    "\n",
    "Useful quantities:\n",
    "\n",
    "- $S_t$ is the state at time $t$, $A_t$ the action at time $t$ landing in state $S_{t + 1}$ with reward $R_{t + 1}$\n",
    "- $\\def\\E{\\mathbb{E}}\\gamma$ is a *discount* factor for future rewards\n",
    "- $G_t$ is the *discounted return*: $G_t = R_{t + 1} + \\gamma R_{t + 2} + \\gamma^2 R_{t + 3} + \\ldots = \\sum_{k = 0}^\\infty \\gamma^k R_{t + k + 1} = \\sum_{k = t + 1}^T \\gamma^{k - t - 1} R_k$ where $T$ can be $\\infty$.\n",
    "- The *value function $v_\\pi$ for policy $\\pi$* : $v_\\pi(s) = \\E_\\pi[G_t|S_t = s]$\n",
    "- The *action-value function $q_\\pi$ for policy $\\pi$* : $q_\\pi(s, a) = \\E_\\pi[G_t|S_t = s, A_t = a]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's imagine a grid where the following rules occur:\n",
    "\n",
    "- Movements against the walls give a reward of $-1$\n",
    "- Walking on $(1, 4)$ gives a reward of $10$ and teleports to $(1, 0)$\n",
    "- Walking on $(3, 4)$ gives a reward of $5$ and teleports to $(3, 2)$\n",
    "- Every other movement gives no reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "[0 0 0 0 0]\n",
      "[0 0 0 0 0]\n",
      "[0 0 0 0 0]\n",
      "[0 0 0 0 0]\n",
      "^^^^^\n",
      "^^^^^\n",
      "^^^^^\n",
      "^^^^^\n",
      "^^^^^\n"
     ]
    }
   ],
   "source": [
    "actions = [(0, 1), (1, 0), (0, -1), (-1, 0)]\n",
    "states = {(x, y) for x in range(5) for y in range(5)}\n",
    "policy = {s: [0.25, 0.25, 0.25, 0.25] for s in states}\n",
    "value = {s: 0 for s in states}\n",
    "\n",
    "def next_(s, a):\n",
    "    if s == (1, 4):\n",
    "        return 10, (1, 0)\n",
    "    if s == (3, 4):\n",
    "        return 5, (3, 2)\n",
    "    x = s[0] + actions[a][0]\n",
    "    y = s[1] + actions[a][1]\n",
    "    if 0 <= x < 5 and 0 <= y < 5:\n",
    "        return 0, (x, y)\n",
    "    else:\n",
    "        return -1, s\n",
    "\n",
    "def plot(policy, value):\n",
    "    for y in range(4, -1, -1):\n",
    "        print(np.round([value[x, y] for x in range(5)], 1))\n",
    "    for y in range(4, -1, -1):\n",
    "        print(''.join(['^>v<'[np.argmax(policy[(x, y)])] for x in range(5)]))\n",
    "\n",
    "GAMMA = 0.9\n",
    "\n",
    "plot(policy, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy evaluation\n",
    "\n",
    "Given the recursive expression $$v_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s', r} p(s', r|s, a) [r + \\gamma v_\\pi(s')]$$\n",
    "\n",
    "Compute the state value function $v_\\pi$ for the equiprobable random policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(policy, NB_STEPS=100):\n",
    "    # Your code here\n",
    "    return value\n",
    "\n",
    "value = eval_policy(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy improvement\n",
    "\n",
    "Given this new value function, we can improve our policy: $$ \\DeclareMathOperator*{\\argmax}{argmax} \\pi'(s) = \\argmax_a q(s, a) = \\argmax_a \\sum_{s', r} p(s', r|s, a) [r + \\gamma v_\\pi(s)]. $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement_is_stable(policy, value):\n",
    "    # Your code here\n",
    "    return is_stable  # Should return True if the policy did not change at all\n",
    "\n",
    "policy_improvement_is_stable(policy, value)\n",
    "plot(policy, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until stability, alternatively do policy evaluation then policy improvement. What is the best policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(policy, MAX_STEPS=30):\n",
    "    pass\n",
    "\n",
    "nb_steps, policy, value = policy_iteration(policy)\n",
    "print('Converged in', nb_steps, 'steps')\n",
    "plot(policy, value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
